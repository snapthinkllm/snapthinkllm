# ğŸš€ SnapThink LLM â€“ Local AI Chat App

SnapThink is a powerful local chat UI powered by [Ollama](https://ollama.com) and Electron. It supports model management, chat session history, markdown rendering, document summarization, and retrieval-augmented generation (RAG) â€“ all locally on your machine.

---

## ğŸ§° Getting Started

1. **Clone the repo**
2. `npm install`
3. `npm run build`
4. `npm run dev`

âš ï¸ You must have **[Ollama](https://ollama.com/download)** CLI installed and available in your system PATH.

---

## âœ… Features Overview

### ğŸ’¬ Core Chat Features

| Feature | Description |
|--------|-------------|
| ğŸ’¡ **Chat Interface** | ChatGPT-style UI with markdown, role tags, timestamps |
| âŒ¨ï¸ **Input Box** | Text input with enter-to-send and button controls |
| ğŸ“¤ **Local LLM API** | Uses `ollama`'s `/api/chat` endpoint |
| ğŸŒ— **Dark Mode** | Toggleable theme, stored in localStorage |

---

### ğŸ’¾ Chat Session Management

| Feature | Description |
|--------|-------------|
| ğŸ“ **Save/Load Chats** | Each session saved as JSON under `userData/chats` |
| ğŸ“‹ **Sidebar with Sessions** | Displays named chats with rename + delete support |
| ğŸ“ **Rename Session** | Edits `chats.json` manifest file |
| ğŸ—‘ï¸ **Delete Session** | Removes chat + manifest entry |
| ğŸ“¤ **Export / ğŸ“¥ Import Chat** | Backup and restore individual chat sessions |

---

### ğŸ§  Model Management

| Feature | Description |
|--------|-------------|
| ğŸ“¦ **Model Selector Screen** | Shows known models + hardware recommendations |
| ğŸ“¥ **Pull Models from Ollama** | Use custom model names (e.g., `mistral:7b-instruct`) |
| â³ **Download Progress Modal** | Tracks and displays status with cancel support |
| ğŸ”„ **Cancel and resume download** |  next download with resumes where previous one left|
| âœ… **Downloaded Models Listing** | Separates downloaded models from suggested ones |
| âš ï¸ **Manual Override** | Allows selecting incompatible models |
| ğŸ§  **Hardware Detection** | Auto-detects RAM/VRAM via `systeminformation` |

---

### ğŸ§¾ Document Summarization

| Feature | Description |
|--------|-------------|
| ğŸ“„ **PDF Parsing** | Reads and parses PDFs using `pdf-parse` |
| ğŸ§© **Chunking** | Splits large docs into manageable pieces |
| ğŸ§  **Summarization** | Summarizes each chunk locally using LLM |
| ğŸ“Š **Session Integration** | Injects summary into current chat |

---

### ğŸ” Retrieval-Augmented Generation (RAG)

| Feature | Description |
|--------|-------------|
| ğŸ“‚ **Per-Session Document Indexing** | Uploaded files are chunked and embedded |
| ğŸ§  **Contextual Answers** | Matched chunks are passed to prompt |
| ğŸ“„ **Show Sources Toggle** | Collapsible "Sources" section under answers |
| ğŸ”’ **Session Isolation** | Document context is scoped to each session |

---

## ğŸ›  Developer Notes

| Feature | Description |
|--------|-------------|
| âš™ï¸ **Electron Integration** | Secure `ipcMain` + `preload.cjs` bridge |
| ğŸ“‚ **Open Chat Folder** | Shows folder in file explorer |
| ğŸ”„ **Model List Parsing** | Compatible with older Ollama CLI via fallback |
| ğŸ›  **Pluggable Model Registry** | Easily expand model list with JSON/JS config |

---

## ğŸ§ª Roadmap

- [ ] Multi-file RAG per session
- [ ] Model config editor UI
- [ ] Settings panel (GPU usage, system caps)
- [ ] Export full chat history

---

## ğŸ“ Notes

- Models are stored and served locally via Ollama.
- This app doesn't send any data to the internet unless the LLM model you're using does so (e.g., community-created models).
- All features run 100% offline.



Generate Exe:
npm install --save-dev electron-builder 
npm run build
npm run dist